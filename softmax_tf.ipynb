{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector() #Face detector\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N = 1000\n",
    "D = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_image = []\n",
    "training_label_full = []\n",
    "test_image = []\n",
    "test_label_full = []\n",
    "\n",
    "\n",
    "for index, row in D.iterrows():\n",
    "    if row['Usage'] == 'Training':\n",
    "        training_image.append(np.reshape(np.array(row['pixels'].split(), dtype=np.uint8), (48, 48)))\n",
    "        training_label_full.append(row['emotion'])\n",
    "    else:\n",
    "        test_image.append(np.reshape(np.array(row['pixels'].split(), dtype=np.uint8), (48, 48)))\n",
    "        test_label_full.append(row['emotion'])\n",
    "        \n",
    "# returns feature vector from an image\n",
    "def get_landmarks(_image):\n",
    "    image = _image.copy()\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "                \n",
    "#         xmean = np.mean(xlist) #Get the mean of both axes to determine centre of gravity\n",
    "#         ymean = np.mean(ylist)\n",
    "        xmean = xlist[29]\n",
    "        ymean = ylist[29]\n",
    "        xcentral = [(x-xmean) for x in xlist] #get distance between each point and the central point in both axes\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        \n",
    "        # point 29 - nose tip\n",
    "        # point 26 - middle point b/w two eyes\n",
    "        cv.circle(image, (int(xlist[29]), int(ylist[29])), 1, (0,0,255))\n",
    "        \n",
    "        angle_nose = np.arctan2((ylist[26]-ymean), (xlist[26]-xmean)) * 180 / np.pi\n",
    "        if angle_nose < 0:\n",
    "            angle_nose += 90\n",
    "        else:\n",
    "            angle_nose -= 90\n",
    "            \n",
    "#         print(angle_nose)\n",
    "\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(x) #Add the coordinates relative to the centre of gravity\n",
    "            landmarks_vectorised.append(y)\n",
    "\n",
    "            #Get the euclidean distance between each point and the centre point (the vector length)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorised.append(dist)\n",
    "\n",
    "            #Get the angle the vector describes relative to the image, corrected for the offset that the nosebrigde has when the face is not perfectly horizontal\n",
    "            anglerelative = (np.arctan2((z - ymean), (w - xmean)) * 180 / np.pi) - angle_nose\n",
    "            landmarks_vectorised.append(anglerelative)\n",
    "        \n",
    "    if len(detections) < 1: \n",
    "        landmarks_vectorised = \"error\"     \n",
    "    \n",
    "    return landmarks_vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train - ', 93.32631802558899)\n",
      "('test - ', 116.11344814300537)\n",
      "(19863, 19863)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    test_data = []\n",
    "    training_label = []\n",
    "    test_label = []\n",
    "    \n",
    "    #Append data to training and prediction list, and generate labels 0-7\n",
    "    index = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for item in training_image:\n",
    "        landmarks_vectorised = get_landmarks(item)\n",
    "        if landmarks_vectorised == \"error\":\n",
    "            pass\n",
    "        else:\n",
    "            training_data.append(landmarks_vectorised) #append image array to training data list\n",
    "            training_label.append(training_label_full[index])\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    print('train - ', time.time() - start_time)\n",
    "    \n",
    "    index = 0\n",
    "    for item in test_image:\n",
    "        landmarks_vectorised = get_landmarks(item)\n",
    "        if landmarks_vectorised == \"error\":\n",
    "            pass\n",
    "        else:\n",
    "            test_data.append(landmarks_vectorised)\n",
    "            test_label.append(test_label_full[index])\n",
    "            \n",
    "        index += 1\n",
    "        \n",
    "    print('test - ', time.time() - start_time)\n",
    "\n",
    "    return training_data, test_data, training_label, test_label\n",
    "\n",
    "# --------------------------------------------------------\n",
    "\n",
    "train_data, test_data, training_label, test_label = make_sets()\n",
    "\n",
    "print(len(train_data), len(training_label))\n",
    "\n",
    "# pd.DataFrame(train_data)\n",
    "# pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data)\n",
    "training_label = np.array(training_label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "train_label_one_hot = tf.Session().run(tf.one_hot(training_label, depth=7, on_value=1, off_value=0))\n",
    "test_label_one_hot = tf.Session().run(tf.one_hot(test_label, depth=7, on_value=1, off_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('tf init time - ', 0.16815495491027832)\n"
     ]
    }
   ],
   "source": [
    "# tensorflow\n",
    "no_of_features = 268\n",
    "no_of_classes = 7\n",
    "x = tf.placeholder(tf.float32, [None, no_of_features])\n",
    "W = tf.Variable(tf.zeros([no_of_features, no_of_classes]))\n",
    "b = tf.Variable(tf.zeros([no_of_classes]))\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "# Define loss and optimizer\n",
    "y_ = tf.placeholder(tf.float32, [None, no_of_classes])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "start_time = time.time()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "print('tf init time - ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "('tf run time - ', 48.75536608695984)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for _ in range(1000):\n",
    "#     batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    if _ % 50 == 0:\n",
    "        print(_)\n",
    "    sess.run(train_step, feed_dict={x: train_data, y_: train_label_one_hot})\n",
    "\n",
    "print('tf run time - ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24549\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: test_data, y_: test_label_one_hot}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
